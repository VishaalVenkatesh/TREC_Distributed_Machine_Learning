{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn.model_selection as model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-0ca7c333717b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'10_Data/20_Extracted Tweets/10_2018 Train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '10_Data/20_Extracted Tweets/10_2018 Train'"
     ],
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '10_Data/20_Extracted Tweets/10_2018 Train'",
     "output_type": "error"
    }
   ],
   "source": [
    "os.chdir('10_Data/20_Extracted Tweets/10_2018 Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\Vishaal\\\\Documents\\\\GitHub\\\\TREC_Distributed_Machine_Learning\\\\TREC\\\\10_Data\\\\20_Extracted Tweets\\\\10_2018 Train'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 15
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Loading Earthquake and flood data from 2018 train. These are the only ones with critical tweets\n",
    "'''\n",
    "df1 = pd.read_csv('floods_TREC_2018_train.csv')\n",
    "df2 = pd.read_csv('Earthquake_TREC_2018_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "os.chdir('../15_2018 Test')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Loading our test 2018 tweets from eathquakes and floods. These have a decent amount of critical tweets.\n",
    "    We did not include attacks as a considerable amount of work has been done on that before.\n",
    "'''\n",
    "df3 = pd.read_csv('Earthquake_TREC_2018_test.csv')\n",
    "df4 = pd.read_csv('Floods_TREC_2018_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Combining all into one big data frame\n",
    "'''\n",
    "df_c = pd.DataFrame()\n",
    "df_c['Tweet'] = pd.concat([df1['Tweet'] , df2['Tweet'], df3['Tweet'], df4['Tweet'] ])\n",
    "df_c['Priority'] = pd.concat([df1['Priority'] , df2['Priority'], df3['Priority'], df4['Priority']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "(52, 2)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 19
    }
   ],
   "source": [
    "'''\n",
    "    52 Critical tweets out of 7432\n",
    "'''\n",
    "(df_c[df_c['Priority']=='Critical']).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Creating a categorical variable to keep label critical tweets as 1 and 0 otherwise\n",
    "'''\n",
    "def to_categorical(df_c):\n",
    "    t = []\n",
    "    for element in df_c['Priority']:\n",
    "        if element =='Critical':\n",
    "            t.append(1)\n",
    "        else:\n",
    "            t.append(0)\n",
    "        \n",
    "    t = np.array(t)\n",
    "    df_c['Target'] = t\n",
    "\n",
    "    df_c['Target'] = df_c['Target'].astype('category')\n",
    "    t = df_c['Target']\n",
    "    del df_c['Target']\n",
    "    return (t)\n",
    "\n",
    "t = to_categorical(df_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Creating a function to input lemmatized text to possibly another function that outputs the tfidf in a csv format.\n",
    "    We could also simply use the output from this funtion in an tfidf format (no csv) and train a model.\n",
    "'''\n",
    "def preProcess(df):\n",
    "    df['Tweet'] = df['Tweet'].astype('str')\n",
    "    \n",
    "    token_array = []\n",
    "    for tweet in df['Tweet']:\n",
    "        token_tweet = word_tokenize(tweet)\n",
    "        token_array.append(token_tweet)\n",
    "        \n",
    "    stop_words=set(stopwords.words(\"english\"))\n",
    "    filtered_token_array=[]\n",
    "    for tweet in token_array:\n",
    "        filtered_tweet = []\n",
    "        for word in tweet:\n",
    "                if word not in stop_words:\n",
    "                    filtered_tweet.append(word)\n",
    "        filtered_token_array.append(filtered_tweet)\n",
    "        \n",
    "    lem = WordNetLemmatizer()\n",
    "    stem = PorterStemmer()\n",
    "\n",
    "    lemmatized_array=[]\n",
    "    for tweet in filtered_token_array:\n",
    "        lemmatized_tweet = []\n",
    "        for word in tweet:\n",
    "            lemmatized_tweet.append(lem.lemmatize(word,'v'))\n",
    "        lemmatized_array.append(lemmatized_tweet)\n",
    "    \n",
    "    lemmatized_array_join = []\n",
    "    for element in lemmatized_array:\n",
    "        lemmatized_array_join.append(''.join(element))\n",
    "        \n",
    "    return (lemmatized_array_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    OPTIONAL - We use this to convert the tfidf into CSV format. \n",
    "    \n",
    "    Output is a combined dataframe - use pd.to_csv to specify save location \n",
    "    \n",
    "    NOTE - corpus is tfidf in non-array format\n",
    "'''\n",
    "def tfidf_to_CSV(df, corpus):\n",
    "    tf=TfidfVectorizer()\n",
    "    text_tf= tf.fit_transform(corpus)\n",
    "    text_tf_dense = text_tf.todense()\n",
    "    \n",
    "    words = tf.get_feature_names()\n",
    "    M = text_tf.tolil()\n",
    "    l_features = []\n",
    "    for i in range(M.shape[0]):\n",
    "        l_features.append(np.array(M[i].todense())[0])\n",
    "        \n",
    "    df_features = pd.DataFrame(l_features)\n",
    "    \n",
    "    df_combined = pd.concat([df,df_features], axis=1)\n",
    "    \n",
    "    cols = list(df.columns) + words\n",
    "    \n",
    "    df_combined_1 = df_combined.rename(columns={x:y for x,y in zip(df_combined.columns,cols)})\n",
    "    \n",
    "    return (df_combined_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    DTM to get TF-IDF features\n",
    "'''\n",
    "tf=TfidfVectorizer()\n",
    "lemmatized_array_join = preProcess(df_c)\n",
    "text_tf= tf.fit_transform(lemmatized_array_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Converting TF-IDF to list of lists. Then we play around with the datatypes to get features as a dense\n",
    "    list of numpy arrays l_features. We also get the actual word names that are used as features.\n",
    "    \n",
    "    Use only if you need the csv output. NOT REQUIRED TO TRAIN MODEL\n",
    "'''\n",
    "words = tf.get_feature_names()\n",
    "M = text_tf.tolil()\n",
    "l_features = []\n",
    "for i in range(M.shape[0]):\n",
    "    l_features.append(np.array(M[i].todense())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Converting DTM to array. REQUIRED TO DIRECTLY TRAIN SVM\n",
    "'''\n",
    "text_tf= tf.fit_transform(lemmatized_array_join).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Recall Score =  0.4444444444444444\nPrecision Score =  0.0075046904315197\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "'''\n",
    "    * Train SVM\n",
    "    * ‘optimal’: eta = 1.0 / (alpha * (t + t0)) where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
    "    * This model is trained directly using the tfidf in the matrix form.\n",
    "'''\n",
    "X_train, X_val, Y_train, Y_val = model_selection.train_test_split(text_tf, t, test_size=0.2, random_state=100)\n",
    "clf = make_pipeline(StandardScaler(with_mean=False), SGDClassifier(loss = 'squared_loss', alpha = 0.0001, max_iter=10000000, tol=1e-3\n",
    "                                                   , shuffle = True, learning_rate = 'optimal', penalty='l1'))\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "print('Recall Score = ', metrics.recall_score(Y_val, y_pred))\n",
    "\n",
    "print('Precision Score = ', metrics.precision_score(Y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[947, 529],\n       [  5,   4]], dtype=int64)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 30
    }
   ],
   "source": [
    "metrics.confusion_matrix(Y_val, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "'''\n",
    "    Let us now balance the dataset and see what happens. We have 52 critical tweets so lets balance that with\n",
    "    200 low priority tweets.\n",
    "'''\n",
    "df_low = df_c[df_c['Priority'] == 'Low'].sample(1000)\n",
    "df_crit = df_c[df_c['Priority'] == 'Critical']\n",
    "df_lc = pd.concat([df_low, df_crit])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "'''\n",
    "    Convert to categorical\n",
    "'''\n",
    "t_lc = to_categorical(df_lc)\n",
    "'''\n",
    "    Getting a DTM of tf-idf features\n",
    "'''\n",
    "tf=TfidfVectorizer()\n",
    "lemmatized_array_join = preProcess(df_lc)\n",
    "text_lc= tf.fit_transform(lemmatized_array_join)\n",
    "'''\n",
    "    Converting DTM to array. REQUIRED TO DIRECTLY TRAIN SVM\n",
    "'''\n",
    "text_lc= tf.fit_transform(lemmatized_array_join).toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Recall Score =  0.7692307692307693\nPrecision Score 0.08\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "X_train, X_val, Y_train, Y_val = model_selection.train_test_split(text_lc, t_lc, test_size=0.2, random_state=100)\n",
    "clf = make_pipeline(StandardScaler(with_mean=False), SGDClassifier(loss = 'squared_loss', alpha = 0.0001, max_iter=10000000, tol=1e-3\n",
    "                                                   , shuffle = True, learning_rate = 'optimal', penalty='l1'))\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "print('Recall Score = ', metrics.recall_score(Y_val, y_pred))\n",
    "\n",
    "print('Precision Score', metrics.precision_score(Y_val, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[32,  6],\n       [11,  2]], dtype=int64)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 155
    }
   ],
   "source": [
    "metrics.confusion_matrix(Y_val, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}