{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.utils import shuffle\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '../10_Data/20_Extracted Tweets/10_2018 Train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-149-156e58c2139c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../10_Data/20_Extracted Tweets/10_2018 Train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '../10_Data/20_Extracted Tweets/10_2018 Train'"
     ]
    }
   ],
   "source": [
    "os.chdir('../10_Data/20_Extracted Tweets/10_2018 Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Vishaal\\\\Documents\\\\GitHub\\\\TREC_Distributed_Machine_Learning\\\\TREC\\\\10_Data\\\\20_Extracted Tweets\\\\10_2018 Train'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Loading Earthquake and flood data from 2018 train. These are the only ones with critical tweets\n",
    "'''\n",
    "df1 = pd.read_csv('flood_TREC_2018_train.csv')\n",
    "df2 = pd.read_csv('Earthquake_TREC_2018_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir('../15_2018 Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Loading our test 2018 tweets from eathquakes and floods. These have a decent amount of critical tweets.\n",
    "    We did not include attacks as a considerable amount of work has been done on that before.\n",
    "'''\n",
    "df3 = pd.read_csv('Earthquake_TREC_2018_test.csv')\n",
    "df4 = pd.read_csv('Floods_TREC_2018_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Combining all into one big data frame\n",
    "'''\n",
    "df_c = pd.DataFrame()\n",
    "df_c['Tweet'] = pd.concat([df1['Tweet'] , df2['Tweet'], df3['Tweet'], df4['Tweet'] ])\n",
    "df_c['Priority'] = pd.concat([df1['Priority'] , df2['Priority'], df3['Priority'], df4['Priority']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55, 2)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    52 Critical tweets out of 7432\n",
    "'''\n",
    "(df_c[df_c['Priority']=='Critical']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Creating a categorical variable to keep label critical tweets as 1 and 0 otherwise\n",
    "'''\n",
    "def to_categorical(df_c):\n",
    "    t = []\n",
    "    for element in df_c['Priority']:\n",
    "        if element =='Critical':\n",
    "            t.append(1)\n",
    "        else:\n",
    "            t.append(0)\n",
    "        \n",
    "    t = np.array(t)\n",
    "    df_c['Target'] = t\n",
    "\n",
    "    df_c['Target'] = df_c['Target'].astype('category')\n",
    "    t = df_c['Target']\n",
    "    del df_c['Target']\n",
    "    return (t)\n",
    "\n",
    "t = to_categorical(df_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Creating a function to input lemmatized text to possibly another function that outputs the tfidf in a csv format.\n",
    "    We could also simply use the output from this funtion in an tfidf format (no csv) and train a model.\n",
    "'''\n",
    "def preProcess(df):\n",
    "    df['Tweet'] = df['Tweet'].astype('str')\n",
    "    \n",
    "    token_array = []\n",
    "    for tweet in df['Tweet']:\n",
    "        token_tweet = word_tokenize(tweet)\n",
    "        token_array.append(token_tweet)\n",
    "        \n",
    "    stop_words=set(stopwords.words(\"english\"))\n",
    "    filtered_token_array=[]\n",
    "    for tweet in token_array:\n",
    "        filtered_tweet = []\n",
    "        for word in tweet:\n",
    "                if word not in stop_words:\n",
    "                    filtered_tweet.append(word)\n",
    "        filtered_token_array.append(filtered_tweet)\n",
    "        \n",
    "    lem = WordNetLemmatizer()\n",
    "    stem = PorterStemmer()\n",
    "\n",
    "    lemmatized_array=[]\n",
    "    for tweet in filtered_token_array:\n",
    "        lemmatized_tweet = []\n",
    "        for word in tweet:\n",
    "            lemmatized_tweet.append(lem.lemmatize(word,'v'))\n",
    "        lemmatized_array.append(lemmatized_tweet)\n",
    "    \n",
    "    lemmatized_array_join = []\n",
    "    for element in lemmatized_array:\n",
    "        lemmatized_array_join.append(' '.join(element))\n",
    "        \n",
    "    return (lemmatized_array_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    OPTIONAL - We use this to convert the tfidf into CSV format. \n",
    "    \n",
    "    Output is a combined dataframe - use pd.to_csv to specify save location \n",
    "    \n",
    "    NOTE - corpus is tfidf in non-array format\n",
    "'''\n",
    "def tfidf_to_CSV(df, corpus):\n",
    "    tf=TfidfVectorizer()\n",
    "    text_tf= tf.fit_transform(corpus)\n",
    "    text_tf_dense = text_tf.todense()\n",
    "    \n",
    "    words = tf.get_feature_names()\n",
    "    M = text_tf.tolil()\n",
    "    l_features = []\n",
    "    for i in range(M.shape[0]):\n",
    "        l_features.append(np.array(M[i].todense())[0])\n",
    "        \n",
    "    df_features = pd.DataFrame(l_features)\n",
    "    \n",
    "    df_combined = pd.concat([df,df_features], axis=1)\n",
    "    \n",
    "    cols = list(df.columns) + words\n",
    "    \n",
    "    df_combined_1 = df_combined.rename(columns={x:y for x,y in zip(df_combined.columns,cols)})\n",
    "    \n",
    "    return (df_combined_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Plot no skill and ROC curve\n",
    "'''\n",
    "def plot_roc_curve(test_y, naive_probs, model_probs):\n",
    "\t# plot naive skill roc curve\n",
    "\tfpr, tpr, _ = roc_curve(test_y, naive_probs)\n",
    "\tplt.plot(fpr, tpr, linestyle='--', label='No Skill')\n",
    "\t# plot model roc curve\n",
    "\tfpr, tpr, _ = roc_curve(test_y, model_probs)\n",
    "\tplt.plot(fpr, tpr, marker='.', label='SVM SGD')\n",
    "\t# axis labels\n",
    "\tplt.xlabel('False Positive Rate')\n",
    "\tplt.ylabel('True Positive Rate')\n",
    "\t# show the legend\n",
    "\tplt.legend()\n",
    "\t# show the plot\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Plot no skill model and PR curve\n",
    "'''\n",
    "def plot_pr_curve(test_y, model_probs):\n",
    "\t# calculate the no skill line as the proportion of the positive class\n",
    "\tno_skill = len(test_y[test_y==1]) / len(test_y)\n",
    "\t# plot the no skill precision-recall curve\n",
    "\tplt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "\t# plot model precision-recall curve\n",
    "\tprecision, recall, _ = precision_recall_curve(test_y, model_probs)\n",
    "\tplt.plot(recall, precision, marker='.', label='SVM SGD')\n",
    "\t# axis labels\n",
    "\tplt.xlabel('Recall')\n",
    "\tplt.ylabel('Precision')\n",
    "\t# show the legend\n",
    "\tplt.legend()\n",
    "\t# show the plot\n",
    "\tplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-157-d659867a0b2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m '''\n\u001b[0;32m      7\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_tf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0ml_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text_tf' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Converting TF-IDF to list of lists. Then we play around with the datatypes to get features as a dense\n",
    "    list of numpy arrays l_features. We also get the actual word names that are used as features.\n",
    "    \n",
    "    Use only if you need the csv output. NOT REQUIRED TO TRAIN MODEL\n",
    "'''\n",
    "words = tf.get_feature_names()\n",
    "M = text_tf.tolil()\n",
    "l_features = []\n",
    "for i in range(M.shape[0]):\n",
    "    l_features.append(np.array(M[i].todense())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vishaal\\Anaconda3_Mod\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Vishaal\\Anaconda3_Mod\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Vishaal\\Anaconda3_Mod\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Vishaal\\Anaconda3_Mod\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Vishaal\\Anaconda3_Mod\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Vishaal\\Anaconda3_Mod\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Vishaal\\Anaconda3_Mod\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Vishaal\\Anaconda3_Mod\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Vishaal\\Anaconda3_Mod\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Vishaal\\Anaconda3_Mod\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision =  0.666969696969697\n",
      "Stdev Precision =  0.16247530480338218\n",
      "Mean Recall =  0.49897269397269395\n",
      "Stdev Recall =  0.2171906964323525\n",
      "Mean ROC AUC =  0.7121811521811522\n",
      "Stdev ROC AUC =  0.07799036553048844\n",
      "Mean PR AUC =  0.7344099283865588\n",
      "Stdev PR AUC =  0.10435971624273732\n"
     ]
    }
   ],
   "source": [
    "iter = 10\n",
    "pre = []\n",
    "rec = []\n",
    "auc1 = []\n",
    "auc2 = []\n",
    "\n",
    "for i in range (iter):\n",
    "    '''\n",
    "        Let us now balance the dataset and see what happens. We have 52 critical tweets so lets balance that with\n",
    "        200 low priority tweets.\n",
    "    '''\n",
    "    df_low = df_c[df_c['Priority'] == 'High'].sample(55)\n",
    "    df_crit = df_c[df_c['Priority'] == 'Critical']\n",
    "    df_lc = pd.concat([df_low, df_crit])\n",
    "    df_lc = shuffle(df_lc)\n",
    "    '''\n",
    "        Convert to categorical\n",
    "    '''\n",
    "    t_lc = to_categorical(df_lc)\n",
    "    '''\n",
    "        Getting a DTM of tf-idf features\n",
    "    '''\n",
    "    tf=TfidfVectorizer()\n",
    "    lemmatized_array_join = preProcess(df_lc)\n",
    "    text_lc= tf.fit_transform(lemmatized_array_join)\n",
    "    feature_names = tf.get_feature_names()\n",
    "    dense = text_lc.todense()\n",
    "    denselist = dense.tolist()\n",
    "    df_x = pd.DataFrame(denselist, columns=feature_names)\n",
    "    '''\n",
    "        Converting DTM to array. REQUIRED TO DIRECTLY TRAIN SVM\n",
    "    '''\n",
    "    text_lc= text_lc.toarray()\n",
    "    '''\n",
    "        SGD SVM on Critical-Low Dataset.\n",
    "    '''\n",
    "    X_train, X_val, Y_train, Y_val = model_selection.train_test_split(text_lc, t_lc, test_size=0.2, random_state=100)\n",
    "    '''\n",
    "        Create a Dummy Classifier\n",
    "    '''\n",
    "    naive_model = DummyClassifier(strategy='stratified')\n",
    "    naive_model.fit(X_train, Y_train)\n",
    "    yhat = naive_model.predict_proba(X_val)\n",
    "    naive_probs = yhat[:, 1]\n",
    "    \n",
    "    '''\n",
    "        Actual SVM using SGD\n",
    "    '''\n",
    "    clf = SGDClassifier(loss = 'hinge', alpha = 0.001, max_iter=10000, tol=1e-3\n",
    "                                                       , shuffle = True, learning_rate = 'optimal', penalty='l1')\n",
    "    clf.fit(X_train, Y_train)\n",
    "    y_pred = clf.predict(X_val)\n",
    "    '''\n",
    "        Calibrating above model to yield probabilities. SGD with hinge loss does not spit out prob. It gives the value of \n",
    "        the decision function aka the value of the function across th hyperplane but that does not help us in this case. \n",
    "        Using loss as log or something else will give prob but we want to do SVM and not logistic regression. Finally, \n",
    "        other \n",
    "    '''\n",
    "    model = CalibratedClassifierCV(clf)\n",
    "    model.fit(X_train, Y_train)\n",
    "    proba_cl = model.predict_proba(X_val)[:, 1]\n",
    "    '''\n",
    "        Get recall and precisions scores\n",
    "    '''\n",
    "    rec.append(metrics.recall_score(Y_val, y_pred))\n",
    "    pre.append(metrics.precision_score(Y_val, y_pred))\n",
    "    #print(metrics.confusion_matrix(Y_val, y_pred))\n",
    "    \n",
    "    '''\n",
    "        ROC AUC\n",
    "    '''\n",
    "    roc_auc = roc_auc_score(Y_val, proba_cl)\n",
    "    auc1.append(roc_auc)\n",
    "    #print('ROC AUC %.3f' % roc_auc)\n",
    "    '''\n",
    "        PR AUC\n",
    "    '''\n",
    "    precision, recall, _ = precision_recall_curve(Y_val, proba_cl)\n",
    "    auc_score = auc(recall, precision)\n",
    "    auc2.append(auc_score)\n",
    "\n",
    "print('Mean Precision = ', statistics.mean(pre))\n",
    "print('Stdev Precision = ', statistics.stdev(pre))\n",
    "print('Mean Recall = ', statistics.mean(rec))\n",
    "print('Stdev Recall = ', statistics.stdev(rec))\n",
    "print('Mean ROC AUC = ', statistics.mean(auc1))\n",
    "print('Stdev ROC AUC = ', statistics.stdev(auc1))\n",
    "print('Mean PR AUC = ', statistics.mean(auc2))\n",
    "print('Stdev PR AUC = ', statistics.stdev(auc2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gt</th>\n",
       "      <th>gu5ieblkij</th>\n",
       "      <th>guatemala</th>\n",
       "      <th>gunmen</th>\n",
       "      <th>guy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.247031</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.214601</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.199565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.271235</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.317667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.209712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           gt  gu5ieblkij  guatemala    gunmen       guy\n",
       "0    0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "1    0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "2    0.000000    0.000000   0.231292  0.000000  0.000000\n",
       "3    0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "4    0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "5    0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "6    0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "7    0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "8    0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "9    0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "10   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "11   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "12   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "13   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "14   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "15   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "16   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "17   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "18   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "19   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "20   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "21   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "22   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "23   0.000000    0.000000   0.000000  0.247031  0.000000\n",
       "24   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "25   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "26   0.000000    0.000000   0.000000  0.000000  0.266167\n",
       "27   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "28   0.000000    0.000000   0.214601  0.000000  0.000000\n",
       "29   0.000000    0.000000   0.199565  0.000000  0.000000\n",
       "..        ...         ...        ...       ...       ...\n",
       "80   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "81   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "82   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "83   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "84   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "85   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "86   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "87   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "88   0.000000    0.271235   0.000000  0.000000  0.000000\n",
       "89   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "90   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "91   0.317667    0.000000   0.209712  0.000000  0.000000\n",
       "92   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "93   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "94   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "95   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "96   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "97   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "98   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "99   0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "100  0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "101  0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "102  0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "103  0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "104  0.000000    0.000000   0.210004  0.000000  0.000000\n",
       "105  0.000000    0.000000   0.185233  0.000000  0.000000\n",
       "106  0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "107  0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "108  0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "109  0.000000    0.000000   0.000000  0.000000  0.000000\n",
       "\n",
       "[110 rows x 5 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x.iloc[:, 345:350]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
