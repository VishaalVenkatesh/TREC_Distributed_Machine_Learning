{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "wAGh7tgOpDj7",
    "outputId": "274b0f6d-655d-44b2-9997-2f653829c26e",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn.model_selection as model_selection\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD\n",
    "from numpy import newaxis\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "import io\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import GlobalAveragePooling1D, Conv1D, Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hWDPPiyfA_hx",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "0gitmNze_ojF",
    "outputId": "4cfefae2-03ba-4d67-cc78-eb9a69bc1e06",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vishaal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vishaal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Vishaal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "os.chdir('../10_Data/20_Extracted Tweets/15_2018 Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified: '../15_2018 Test'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d243d53bd417>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../15_2018 Test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: '../15_2018 Test'"
     ]
    }
   ],
   "source": [
    "os.chdir('../15_2018 Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('Earthquake_TREC_2018_test.csv')\n",
    "df2 = pd.read_csv('Floods_TREC_2018_test.csv')\n",
    "#df3 = pd.read_csv('Earthquake_TREC_2018_train.csv')\n",
    "#df4 = pd.read_csv('flood_TREC_2018_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Event</th>\n",
       "      <th>Retweet_Count</th>\n",
       "      <th>Follower_Count</th>\n",
       "      <th>Source</th>\n",
       "      <th>User_Created_at</th>\n",
       "      <th>Tweet_Created_at</th>\n",
       "      <th>User_Language</th>\n",
       "      <th>User_Screen_Name</th>\n",
       "      <th>User_Location</th>\n",
       "      <th>Event_Decrption</th>\n",
       "      <th>Categories</th>\n",
       "      <th>Priority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>510</td>\n",
       "      <td>510</td>\n",
       "      <td>378011169883037697</td>\n",
       "      <td>RT @dlfluegge: Crazy Flooding in Boulder, Colo...</td>\n",
       "      <td>floodColorado2013</td>\n",
       "      <td>10</td>\n",
       "      <td>376</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>2009-07-26 18:15:13</td>\n",
       "      <td>2013-09-12 04:24:20</td>\n",
       "      <td>en</td>\n",
       "      <td>jewmike</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>The 2013 Colorado floods was a natural disaste...</td>\n",
       "      <td>['MultimediaShare']</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>511</td>\n",
       "      <td>511</td>\n",
       "      <td>378020179214491649</td>\n",
       "      <td>Here's the #boulderflood video that's circulat...</td>\n",
       "      <td>floodColorado2013</td>\n",
       "      <td>3</td>\n",
       "      <td>211</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>2010-05-16 21:17:24</td>\n",
       "      <td>2013-09-12 05:00:08</td>\n",
       "      <td>en</td>\n",
       "      <td>KristinsDreams</td>\n",
       "      <td>Denver, CO</td>\n",
       "      <td>The 2013 Colorado floods was a natural disaste...</td>\n",
       "      <td>['MultimediaShare']</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>378026101588496385</td>\n",
       "      <td>RT @passantino: Video: Severe flooding hits ne...</td>\n",
       "      <td>floodColorado2013</td>\n",
       "      <td>3</td>\n",
       "      <td>621</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>2011-02-02 23:20:53</td>\n",
       "      <td>2013-09-12 05:23:40</td>\n",
       "      <td>en</td>\n",
       "      <td>mindfulnurse</td>\n",
       "      <td>Boulder, Colorado</td>\n",
       "      <td>The 2013 Colorado floods was a natural disaste...</td>\n",
       "      <td>['MultimediaShare']</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>513</td>\n",
       "      <td>513</td>\n",
       "      <td>378029784204206080</td>\n",
       "      <td>Crazy Flooding in Boulder, Colorado http://t.c...</td>\n",
       "      <td>floodColorado2013</td>\n",
       "      <td>0</td>\n",
       "      <td>232</td>\n",
       "      <td>&lt;a href=\"http://www.apple.com\" rel=\"nofollow\"&gt;...</td>\n",
       "      <td>2010-04-06 03:59:52</td>\n",
       "      <td>2013-09-12 05:38:18</td>\n",
       "      <td>en</td>\n",
       "      <td>JoshWorldPeace</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The 2013 Colorado floods was a natural disaste...</td>\n",
       "      <td>['MultimediaShare']</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>514</td>\n",
       "      <td>514</td>\n",
       "      <td>378038458024865792</td>\n",
       "      <td>Thoughts and prayers out to those in Boulder w...</td>\n",
       "      <td>floodColorado2013</td>\n",
       "      <td>5</td>\n",
       "      <td>188</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/#!/download/ipad\" ...</td>\n",
       "      <td>2010-02-13 05:31:57</td>\n",
       "      <td>2013-09-12 06:12:46</td>\n",
       "      <td>en</td>\n",
       "      <td>jimatgoleaddog</td>\n",
       "      <td>Lyme &amp; Randolph, NH</td>\n",
       "      <td>The 2013 Colorado floods was a natural disaste...</td>\n",
       "      <td>['Sentiment']</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1                  ID  \\\n",
       "0         510           510  378011169883037697   \n",
       "1         511           511  378020179214491649   \n",
       "2         512           512  378026101588496385   \n",
       "3         513           513  378029784204206080   \n",
       "4         514           514  378038458024865792   \n",
       "\n",
       "                                               Tweet              Event  \\\n",
       "0  RT @dlfluegge: Crazy Flooding in Boulder, Colo...  floodColorado2013   \n",
       "1  Here's the #boulderflood video that's circulat...  floodColorado2013   \n",
       "2  RT @passantino: Video: Severe flooding hits ne...  floodColorado2013   \n",
       "3  Crazy Flooding in Boulder, Colorado http://t.c...  floodColorado2013   \n",
       "4  Thoughts and prayers out to those in Boulder w...  floodColorado2013   \n",
       "\n",
       "   Retweet_Count  Follower_Count  \\\n",
       "0             10             376   \n",
       "1              3             211   \n",
       "2              3             621   \n",
       "3              0             232   \n",
       "4              5             188   \n",
       "\n",
       "                                              Source      User_Created_at  \\\n",
       "0  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...  2009-07-26 18:15:13   \n",
       "1  <a href=\"http://twitter.com/download/iphone\" r...  2010-05-16 21:17:24   \n",
       "2  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...  2011-02-02 23:20:53   \n",
       "3  <a href=\"http://www.apple.com\" rel=\"nofollow\">...  2010-04-06 03:59:52   \n",
       "4  <a href=\"http://twitter.com/#!/download/ipad\" ...  2010-02-13 05:31:57   \n",
       "\n",
       "      Tweet_Created_at User_Language User_Screen_Name        User_Location  \\\n",
       "0  2013-09-12 04:24:20            en          jewmike             Colorado   \n",
       "1  2013-09-12 05:00:08            en   KristinsDreams           Denver, CO   \n",
       "2  2013-09-12 05:23:40            en     mindfulnurse    Boulder, Colorado   \n",
       "3  2013-09-12 05:38:18            en   JoshWorldPeace                  NaN   \n",
       "4  2013-09-12 06:12:46            en   jimatgoleaddog  Lyme & Randolph, NH   \n",
       "\n",
       "                                     Event_Decrption           Categories  \\\n",
       "0  The 2013 Colorado floods was a natural disaste...  ['MultimediaShare']   \n",
       "1  The 2013 Colorado floods was a natural disaste...  ['MultimediaShare']   \n",
       "2  The 2013 Colorado floods was a natural disaste...  ['MultimediaShare']   \n",
       "3  The 2013 Colorado floods was a natural disaste...  ['MultimediaShare']   \n",
       "4  The 2013 Colorado floods was a natural disaste...        ['Sentiment']   \n",
       "\n",
       "  Priority  \n",
       "0     High  \n",
       "1     High  \n",
       "2     High  \n",
       "3     High  \n",
       "4      Low  "
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T4iywK3pAJvM",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Combining all into one big data frame\n",
    "'''\n",
    "df = pd.DataFrame()\n",
    "df['Tweet'] = pd.concat([df1['Tweet'] , df2['Tweet'], df3['Tweet'], df4['Tweet']])\n",
    "df['Priority'] = pd.concat([df1['Priority'] , df2['Priority'], df3['Priority'], df4['Priority']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55, 2)"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Priority']=='Critical'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_low = df[df['Priority'] == 'Low'].sample(55)\n",
    "df_crit = df[df['Priority'] == 'Critical']\n",
    "df_c = pd.concat([df_low, df_crit])\n",
    "df_c = shuffle(df_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = pd.read_csv('Ultimate_Dataframe_TREC_2018_Test.csv')\n",
    "df_c = shuffle(df_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rh3quY1lAbOv",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Creating a categorical variable to keep label critical tweets as 1 and 0 otherwise\n",
    "'''\n",
    "def to_categorical(df_c):\n",
    "    t = []\n",
    "    for element in df_c['Priority']:\n",
    "        if element =='Critical':\n",
    "            t.append(1)\n",
    "        else:\n",
    "            t.append(0)\n",
    "        \n",
    "    t = np.array(t)\n",
    "    df_c['Target'] = t\n",
    "\n",
    "    df_c['Target'] = df_c['Target'].astype('category')\n",
    "    t = df_c['Target']\n",
    "    del df_c['Target']\n",
    "    return (t)\n",
    "\n",
    "t = to_categorical(df_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Qz01zcMJA45Z",
    "outputId": "354c5334-502a-4be0-f773-1fcdbaa51cc2",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Creating a function to input lemmatized text to possibly another function that outputs the tfidf in a csv format.\n",
    "    We could also simply use the output from this funtion in an tfidf format (no csv) and train a model.\n",
    "'''\n",
    "def preProcess(df):\n",
    "    df['Tweet'] = df['Tweet'].astype('str')\n",
    "    \n",
    "    token_array = []\n",
    "    for tweet in df['Tweet']:\n",
    "        token_tweet = word_tokenize(tweet)\n",
    "        token_array.append(token_tweet)\n",
    "        \n",
    "    stop_words=set(stopwords.words(\"english\"))\n",
    "    filtered_token_array=[]\n",
    "    for tweet in token_array:\n",
    "        filtered_tweet = []\n",
    "        for word in tweet:\n",
    "                if word not in stop_words:\n",
    "                    filtered_tweet.append(word)\n",
    "        filtered_token_array.append(filtered_tweet)\n",
    "        \n",
    "    lem = WordNetLemmatizer()\n",
    "    stem = PorterStemmer()\n",
    "\n",
    "    lemmatized_array=[]\n",
    "    for tweet in filtered_token_array:\n",
    "        lemmatized_tweet = []\n",
    "        for word in tweet:\n",
    "            lemmatized_tweet.append(lem.lemmatize(word,'v'))\n",
    "        lemmatized_array.append(lemmatized_tweet)\n",
    "    \n",
    "    lemmatized_array_join = []\n",
    "    for element in lemmatized_array:\n",
    "        lemmatized_array_join.append(' '.join(element))\n",
    "        \n",
    "    return (lemmatized_array_join)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iQn3lpACA6dG",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  We tokenize the lemmatized tweets to then do word embeddings\n",
    "'''\n",
    "l = preProcess(df_c)\n",
    "\n",
    "all_words = []\n",
    "\n",
    "for tweet in l:\n",
    "    tokenize_word = word_tokenize(tweet)\n",
    "    for word in tokenize_word:\n",
    "        all_words.append(word)\n",
    "'''\n",
    "    Getting the unique words out\n",
    "'''\n",
    "unique_words = set(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Getting embedded sentences\n",
    "'''\n",
    "vocab_length = len(unique_words)\n",
    "\n",
    "embedded_sentences = [one_hot(tweet, vocab_length) for tweet in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Making the size of all embeddings equal to the longest one\n",
    "'''\n",
    "word_count = lambda sentence: len(word_tokenize(sentence))\n",
    "longest_sentence = max(l, key=word_count)\n",
    "length_long_sentence = len(word_tokenize(longest_sentence))\n",
    "padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "939"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_109\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_103 (Embedding)    (None, 34, 16)            15024     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_86  (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_180 (Dense)            (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_181 (Dense)            (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 15,169\n",
      "Trainable params: 15,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Model Paramters\n",
    "'''\n",
    "model = keras.Sequential()\n",
    "model.add(Embedding(vocab_length, 16, input_length=padded_sentences.shape[1]))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(8, activation = 'relu', input_shape = (padded_sentences.shape[1],)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 66 samples, validate on 44 samples\n",
      "Epoch 1/20\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.6805 - precision_76: 0.6604 - recall_74: 0.9722 - val_loss: 0.6912 - val_precision_76: 0.4318 - val_recall_74: 1.0000\n",
      "Epoch 2/20\n",
      "66/66 [==============================] - 0s 287us/step - loss: 0.6740 - precision_76: 0.6471 - recall_74: 0.9167 - val_loss: 0.6909 - val_precision_76: 0.4318 - val_recall_74: 1.0000\n",
      "Epoch 3/20\n",
      "66/66 [==============================] - 0s 363us/step - loss: 0.6752 - precision_76: 0.6296 - recall_74: 0.9444 - val_loss: 0.6882 - val_precision_76: 0.4524 - val_recall_74: 1.0000\n",
      "Epoch 4/20\n",
      "66/66 [==============================] - 0s 393us/step - loss: 0.6705 - precision_76: 0.6939 - recall_74: 0.9444 - val_loss: 0.6873 - val_precision_76: 0.4737 - val_recall_74: 0.9474\n",
      "Epoch 5/20\n",
      "66/66 [==============================] - 0s 514us/step - loss: 0.6593 - precision_76: 0.8140 - recall_74: 0.9722 - val_loss: 0.6844 - val_precision_76: 0.6296 - val_recall_74: 0.8947\n",
      "Epoch 6/20\n",
      "66/66 [==============================] - 0s 468us/step - loss: 0.6606 - precision_76: 0.8250 - recall_74: 0.9167 - val_loss: 0.6824 - val_precision_76: 0.6667 - val_recall_74: 0.8421\n",
      "Epoch 7/20\n",
      "66/66 [==============================] - 0s 438us/step - loss: 0.6527 - precision_76: 0.8500 - recall_74: 0.9444 - val_loss: 0.6812 - val_precision_76: 0.6842 - val_recall_74: 0.6842\n",
      "Epoch 8/20\n",
      "66/66 [==============================] - 0s 363us/step - loss: 0.6578 - precision_76: 0.8095 - recall_74: 0.9444 - val_loss: 0.6774 - val_precision_76: 0.7333 - val_recall_74: 0.5789\n",
      "Epoch 9/20\n",
      "66/66 [==============================] - 0s 423us/step - loss: 0.6465 - precision_76: 0.8649 - recall_74: 0.8889 - val_loss: 0.6757 - val_precision_76: 0.7333 - val_recall_74: 0.5789\n",
      "Epoch 10/20\n",
      "66/66 [==============================] - 0s 332us/step - loss: 0.6429 - precision_76: 0.8049 - recall_74: 0.9167 - val_loss: 0.6755 - val_precision_76: 0.6667 - val_recall_74: 0.6316\n",
      "Epoch 11/20\n",
      "66/66 [==============================] - 0s 348us/step - loss: 0.6356 - precision_76: 0.8649 - recall_74: 0.8889 - val_loss: 0.6723 - val_precision_76: 0.7333 - val_recall_74: 0.5789\n",
      "Epoch 12/20\n",
      "66/66 [==============================] - 0s 393us/step - loss: 0.6328 - precision_76: 0.9167 - recall_74: 0.9167 - val_loss: 0.6695 - val_precision_76: 0.7857 - val_recall_74: 0.5789\n",
      "Epoch 13/20\n",
      "66/66 [==============================] - 0s 335us/step - loss: 0.6315 - precision_76: 0.8611 - recall_74: 0.8611 - val_loss: 0.6677 - val_precision_76: 0.7857 - val_recall_74: 0.5789\n",
      "Epoch 14/20\n",
      "66/66 [==============================] - 0s 317us/step - loss: 0.6173 - precision_76: 0.8649 - recall_74: 0.8889 - val_loss: 0.6640 - val_precision_76: 0.7857 - val_recall_74: 0.5789\n",
      "Epoch 15/20\n",
      "66/66 [==============================] - 0s 393us/step - loss: 0.6139 - precision_76: 0.8529 - recall_74: 0.8056 - val_loss: 0.6620 - val_precision_76: 0.7857 - val_recall_74: 0.5789\n",
      "Epoch 16/20\n",
      "66/66 [==============================] - 0s 317us/step - loss: 0.6156 - precision_76: 0.8529 - recall_74: 0.8056 - val_loss: 0.6595 - val_precision_76: 0.7857 - val_recall_74: 0.5789\n",
      "Epoch 17/20\n",
      "66/66 [==============================] - 0s 332us/step - loss: 0.5889 - precision_76: 0.9697 - recall_74: 0.8889 - val_loss: 0.6575 - val_precision_76: 0.7857 - val_recall_74: 0.5789\n",
      "Epoch 18/20\n",
      "66/66 [==============================] - 0s 423us/step - loss: 0.5846 - precision_76: 0.9118 - recall_74: 0.8611 - val_loss: 0.6557 - val_precision_76: 0.7857 - val_recall_74: 0.5789\n",
      "Epoch 19/20\n",
      "66/66 [==============================] - 0s 423us/step - loss: 0.5885 - precision_76: 0.9062 - recall_74: 0.8056 - val_loss: 0.6543 - val_precision_76: 0.7857 - val_recall_74: 0.5789\n",
      "Epoch 20/20\n",
      "66/66 [==============================] - 0s 363us/step - loss: 0.5771 - precision_76: 0.9143 - recall_74: 0.8889 - val_loss: 0.6514 - val_precision_76: 0.7857 - val_recall_74: 0.5789\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Compile and fit model\n",
    "'''\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "\n",
    "history = model.fit(padded_sentences, t, batch_size=8, epochs=20, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11105 samples, validate on 4760 samples\n",
      "Epoch 1/100\n",
      "11105/11105 [==============================] - 1s 103us/step - loss: 0.6947 - precision_3: 0.0081 - recall_3: 0.5055 - auc_3: 0.5114 - val_loss: 0.6717 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.4987\n",
      "Epoch 2/100\n",
      "11105/11105 [==============================] - 0s 37us/step - loss: 0.6576 - precision_3: 0.0074 - recall_3: 0.0220 - auc_3: 0.4694 - val_loss: 0.6407 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.5183\n",
      "Epoch 3/100\n",
      "11105/11105 [==============================] - 0s 31us/step - loss: 0.6277 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5300 - val_loss: 0.6117 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.5675\n",
      "Epoch 4/100\n",
      "11105/11105 [==============================] - 0s 31us/step - loss: 0.5985 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5245 - val_loss: 0.5825 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.5813\n",
      "Epoch 5/100\n",
      "11105/11105 [==============================] - 0s 29us/step - loss: 0.5695 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5095 - val_loss: 0.5528 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.5665\n",
      "Epoch 6/100\n",
      "11105/11105 [==============================] - 0s 30us/step - loss: 0.5400 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5244 - val_loss: 0.5227 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.5766\n",
      "Epoch 7/100\n",
      "11105/11105 [==============================] - 0s 28us/step - loss: 0.5098 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5306 - val_loss: 0.4925 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.5815\n",
      "Epoch 8/100\n",
      "11105/11105 [==============================] - 0s 38us/step - loss: 0.4803 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.4961 - val_loss: 0.4627 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.5928\n",
      "Epoch 9/100\n",
      "11105/11105 [==============================] - 0s 32us/step - loss: 0.4512 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5355 - val_loss: 0.4332 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6010\n",
      "Epoch 10/100\n",
      "11105/11105 [==============================] - 0s 31us/step - loss: 0.4221 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5416 - val_loss: 0.4043 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.5979\n",
      "Epoch 11/100\n",
      "11105/11105 [==============================] - 0s 29us/step - loss: 0.3938 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5391 - val_loss: 0.3761 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6209\n",
      "Epoch 12/100\n",
      "11105/11105 [==============================] - 0s 28us/step - loss: 0.3670 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5321 - val_loss: 0.3489 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6222\n",
      "Epoch 13/100\n",
      "11105/11105 [==============================] - 0s 29us/step - loss: 0.3405 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5403 - val_loss: 0.3227 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6202\n",
      "Epoch 14/100\n",
      "11105/11105 [==============================] - 0s 30us/step - loss: 0.3164 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5837 - val_loss: 0.2975 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6326\n",
      "Epoch 15/100\n",
      "11105/11105 [==============================] - 0s 28us/step - loss: 0.2923 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5595 - val_loss: 0.2736 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6061\n",
      "Epoch 16/100\n",
      "11105/11105 [==============================] - 0s 27us/step - loss: 0.2702 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5186 - val_loss: 0.2510 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6410\n",
      "Epoch 17/100\n",
      "11105/11105 [==============================] - 0s 29us/step - loss: 0.2478 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5712 - val_loss: 0.2297 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6213\n",
      "Epoch 18/100\n",
      "11105/11105 [==============================] - 0s 32us/step - loss: 0.2270 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5889 - val_loss: 0.2098 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6389\n",
      "Epoch 19/100\n",
      "11105/11105 [==============================] - 0s 32us/step - loss: 0.2102 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5847 - val_loss: 0.1913 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6245\n",
      "Epoch 20/100\n",
      "11105/11105 [==============================] - 0s 32us/step - loss: 0.1938 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5056 - val_loss: 0.1742 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6213\n",
      "Epoch 21/100\n",
      "11105/11105 [==============================] - 0s 38us/step - loss: 0.1765 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5352 - val_loss: 0.1584 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6383\n",
      "Epoch 22/100\n",
      "11105/11105 [==============================] - 0s 35us/step - loss: 0.1634 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5268 - val_loss: 0.1440 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6466\n",
      "Epoch 23/100\n",
      "11105/11105 [==============================] - 0s 34us/step - loss: 0.1484 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5322 - val_loss: 0.1309 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6277\n",
      "Epoch 24/100\n",
      "11105/11105 [==============================] - 0s 34us/step - loss: 0.1351 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5885 - val_loss: 0.1190 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6569\n",
      "Epoch 25/100\n",
      "11105/11105 [==============================] - 0s 37us/step - loss: 0.1266 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5326 - val_loss: 0.1083 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6566\n",
      "Epoch 26/100\n",
      "11105/11105 [==============================] - 0s 32us/step - loss: 0.1160 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5294 - val_loss: 0.0989 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6509\n",
      "Epoch 27/100\n",
      "11105/11105 [==============================] - 0s 35us/step - loss: 0.1060 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5517 - val_loss: 0.0904 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6519\n",
      "Epoch 28/100\n",
      "11105/11105 [==============================] - 0s 36us/step - loss: 0.0975 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5644 - val_loss: 0.0829 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6618\n",
      "Epoch 29/100\n",
      "11105/11105 [==============================] - 0s 33us/step - loss: 0.0912 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5590 - val_loss: 0.0763 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6632\n",
      "Epoch 30/100\n",
      "11105/11105 [==============================] - 0s 34us/step - loss: 0.0844 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5934 - val_loss: 0.0706 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6466\n",
      "Epoch 31/100\n",
      "11105/11105 [==============================] - 0s 35us/step - loss: 0.0794 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5762 - val_loss: 0.0656 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6439\n",
      "Epoch 32/100\n",
      "11105/11105 [==============================] - 0s 36us/step - loss: 0.0732 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6278 - val_loss: 0.0614 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6480\n",
      "Epoch 33/100\n",
      "11105/11105 [==============================] - 0s 36us/step - loss: 0.0693 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6064 - val_loss: 0.0579 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100\n",
      "11105/11105 [==============================] - 0s 35us/step - loss: 0.0673 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5726 - val_loss: 0.0549 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6667\n",
      "Epoch 35/100\n",
      "11105/11105 [==============================] - 0s 32us/step - loss: 0.0632 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5919 - val_loss: 0.0524 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6840\n",
      "Epoch 36/100\n",
      "11105/11105 [==============================] - 0s 37us/step - loss: 0.0611 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5706 - val_loss: 0.0504 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6516\n",
      "Epoch 37/100\n",
      "11105/11105 [==============================] - 0s 33us/step - loss: 0.0586 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5588 - val_loss: 0.0488 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6652\n",
      "Epoch 38/100\n",
      "11105/11105 [==============================] - 0s 43us/step - loss: 0.0570 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5481 - val_loss: 0.0476 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6677\n",
      "Epoch 39/100\n",
      "11105/11105 [==============================] - 0s 37us/step - loss: 0.0553 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5678 - val_loss: 0.0467 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6767\n",
      "Epoch 40/100\n",
      "11105/11105 [==============================] - 0s 32us/step - loss: 0.0521 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6406 - val_loss: 0.0460 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6656\n",
      "Epoch 41/100\n",
      "11105/11105 [==============================] - 0s 37us/step - loss: 0.0528 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5821 - val_loss: 0.0456 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6651\n",
      "Epoch 42/100\n",
      "11105/11105 [==============================] - 0s 34us/step - loss: 0.0522 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5842 - val_loss: 0.0453 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6654\n",
      "Epoch 43/100\n",
      "11105/11105 [==============================] - 0s 36us/step - loss: 0.0497 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6538 - val_loss: 0.0452 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6202\n",
      "Epoch 44/100\n",
      "11105/11105 [==============================] - 0s 32us/step - loss: 0.0512 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5576 - val_loss: 0.0452 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.5917\n",
      "Epoch 45/100\n",
      "11105/11105 [==============================] - 0s 39us/step - loss: 0.0496 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6282 - val_loss: 0.0452 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.5887\n",
      "Epoch 46/100\n",
      "11105/11105 [==============================] - 1s 45us/step - loss: 0.0501 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5966 - val_loss: 0.0452 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6213\n",
      "Epoch 47/100\n",
      "11105/11105 [==============================] - 1s 48us/step - loss: 0.0509 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5895 - val_loss: 0.0452 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6374\n",
      "Epoch 48/100\n",
      "11105/11105 [==============================] - 1s 50us/step - loss: 0.0488 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6370 - val_loss: 0.0453 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6405\n",
      "Epoch 49/100\n",
      "11105/11105 [==============================] - 0s 41us/step - loss: 0.0491 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6033 - val_loss: 0.0453 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6238\n",
      "Epoch 50/100\n",
      "11105/11105 [==============================] - 0s 41us/step - loss: 0.0484 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6431 - val_loss: 0.0453 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6424\n",
      "Epoch 51/100\n",
      "11105/11105 [==============================] - 0s 42us/step - loss: 0.0494 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5901 - val_loss: 0.0453 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6540\n",
      "Epoch 52/100\n",
      "11105/11105 [==============================] - 1s 62us/step - loss: 0.0483 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6353 - val_loss: 0.0453 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6650\n",
      "Epoch 53/100\n",
      "11105/11105 [==============================] - 0s 44us/step - loss: 0.0492 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5962 - val_loss: 0.0453 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6773\n",
      "Epoch 54/100\n",
      "11105/11105 [==============================] - 0s 41us/step - loss: 0.0474 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6635 - val_loss: 0.0453 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6766\n",
      "Epoch 55/100\n",
      "11105/11105 [==============================] - 0s 34us/step - loss: 0.0487 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6072 - val_loss: 0.0452 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6794\n",
      "Epoch 56/100\n",
      "11105/11105 [==============================] - 0s 41us/step - loss: 0.0505 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5717 - val_loss: 0.0452 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6764\n",
      "Epoch 57/100\n",
      "11105/11105 [==============================] - 0s 43us/step - loss: 0.0504 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.5851 - val_loss: 0.0451 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6630\n",
      "Epoch 58/100\n",
      "11105/11105 [==============================] - 0s 44us/step - loss: 0.0488 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6196 - val_loss: 0.0451 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6676\n",
      "Epoch 59/100\n",
      "11105/11105 [==============================] - 0s 42us/step - loss: 0.0489 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6161 - val_loss: 0.0450 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6596\n",
      "Epoch 60/100\n",
      "11105/11105 [==============================] - 0s 43us/step - loss: 0.0495 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6120 - val_loss: 0.0449 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6661\n",
      "Epoch 61/100\n",
      "11105/11105 [==============================] - 0s 44us/step - loss: 0.0472 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6642 - val_loss: 0.0449 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6706\n",
      "Epoch 62/100\n",
      "11105/11105 [==============================] - 1s 52us/step - loss: 0.0472 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6720 - val_loss: 0.0449 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6791\n",
      "Epoch 63/100\n",
      "11105/11105 [==============================] - 1s 57us/step - loss: 0.0474 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6716 - val_loss: 0.0449 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6889\n",
      "Epoch 64/100\n",
      "11105/11105 [==============================] - 0s 40us/step - loss: 0.0491 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6165 - val_loss: 0.0449 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6998\n",
      "Epoch 65/100\n",
      "11105/11105 [==============================] - 1s 54us/step - loss: 0.0484 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6253 - val_loss: 0.0448 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6907\n",
      "Epoch 66/100\n",
      "11105/11105 [==============================] - 0s 42us/step - loss: 0.0484 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6198 - val_loss: 0.0448 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100\n",
      "11105/11105 [==============================] - 1s 48us/step - loss: 0.0480 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6571 - val_loss: 0.0447 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6926\n",
      "Epoch 68/100\n",
      "11105/11105 [==============================] - 1s 48us/step - loss: 0.0484 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6429 - val_loss: 0.0447 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6934\n",
      "Epoch 69/100\n",
      "11105/11105 [==============================] - 1s 48us/step - loss: 0.0469 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6762 - val_loss: 0.0447 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6940\n",
      "Epoch 70/100\n",
      "11105/11105 [==============================] - 1s 46us/step - loss: 0.0463 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6960 - val_loss: 0.0446 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6992\n",
      "Epoch 71/100\n",
      "11105/11105 [==============================] - 0s 39us/step - loss: 0.0468 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6983 - val_loss: 0.0446 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7105\n",
      "Epoch 72/100\n",
      "11105/11105 [==============================] - 1s 48us/step - loss: 0.0469 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6622 - val_loss: 0.0446 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7184\n",
      "Epoch 73/100\n",
      "11105/11105 [==============================] - 0s 41us/step - loss: 0.0474 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6565 - val_loss: 0.0446 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7197\n",
      "Epoch 74/100\n",
      "11105/11105 [==============================] - 0s 41us/step - loss: 0.0471 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6636 - val_loss: 0.0445 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7153\n",
      "Epoch 75/100\n",
      "11105/11105 [==============================] - 0s 43us/step - loss: 0.0464 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6949 - val_loss: 0.0445 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7081\n",
      "Epoch 76/100\n",
      "11105/11105 [==============================] - 1s 48us/step - loss: 0.0465 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6841 - val_loss: 0.0444 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7117\n",
      "Epoch 77/100\n",
      "11105/11105 [==============================] - 1s 47us/step - loss: 0.0466 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6719 - val_loss: 0.0444 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.6978\n",
      "Epoch 78/100\n",
      "11105/11105 [==============================] - 0s 40us/step - loss: 0.0475 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6507 - val_loss: 0.0443 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7158\n",
      "Epoch 79/100\n",
      "11105/11105 [==============================] - 1s 49us/step - loss: 0.0459 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6657 - val_loss: 0.0443 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7010\n",
      "Epoch 80/100\n",
      "11105/11105 [==============================] - 0s 39us/step - loss: 0.0455 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.7024 - val_loss: 0.0443 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7117\n",
      "Epoch 81/100\n",
      "11105/11105 [==============================] - 1s 51us/step - loss: 0.0454 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.7299 - val_loss: 0.0443 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7137\n",
      "Epoch 82/100\n",
      "11105/11105 [==============================] - 1s 50us/step - loss: 0.0456 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.7169 - val_loss: 0.0442 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7163\n",
      "Epoch 83/100\n",
      "11105/11105 [==============================] - 1s 47us/step - loss: 0.0469 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6538 - val_loss: 0.0442 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7141\n",
      "Epoch 84/100\n",
      "11105/11105 [==============================] - 0s 44us/step - loss: 0.0449 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.7259 - val_loss: 0.0442 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7240\n",
      "Epoch 85/100\n",
      "11105/11105 [==============================] - 1s 50us/step - loss: 0.0466 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6692 - val_loss: 0.0441 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7415\n",
      "Epoch 86/100\n",
      "11105/11105 [==============================] - 0s 35us/step - loss: 0.0452 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.7326 - val_loss: 0.0441 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7300\n",
      "Epoch 87/100\n",
      "11105/11105 [==============================] - 0s 38us/step - loss: 0.0466 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6846 - val_loss: 0.0440 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7331\n",
      "Epoch 88/100\n",
      "11105/11105 [==============================] - 0s 35us/step - loss: 0.0445 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.7441 - val_loss: 0.0440 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7189\n",
      "Epoch 89/100\n",
      "11105/11105 [==============================] - 0s 39us/step - loss: 0.0451 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.7236 - val_loss: 0.0440 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7252\n",
      "Epoch 90/100\n",
      "11105/11105 [==============================] - 0s 37us/step - loss: 0.0455 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.6939 - val_loss: 0.0439 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7454\n",
      "Epoch 91/100\n",
      "11105/11105 [==============================] - 0s 36us/step - loss: 0.0454 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.7244 - val_loss: 0.0439 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7624\n",
      "Epoch 92/100\n",
      "11105/11105 [==============================] - 0s 37us/step - loss: 0.0453 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.7003 - val_loss: 0.0438 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7571\n",
      "Epoch 93/100\n",
      "11105/11105 [==============================] - 0s 38us/step - loss: 0.0449 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.7398 - val_loss: 0.0438 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7622\n",
      "Epoch 94/100\n",
      "11105/11105 [==============================] - 0s 39us/step - loss: 0.0447 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.7442 - val_loss: 0.0437 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7723\n",
      "Epoch 95/100\n",
      "11105/11105 [==============================] - 0s 40us/step - loss: 0.0446 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.7524 - val_loss: 0.0437 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7714\n",
      "Epoch 96/100\n",
      "11105/11105 [==============================] - 0s 35us/step - loss: 0.0445 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.7509 - val_loss: 0.0437 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7730\n",
      "Epoch 97/100\n",
      "11105/11105 [==============================] - 0s 36us/step - loss: 0.0443 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.7481 - val_loss: 0.0437 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7865\n",
      "Epoch 98/100\n",
      "11105/11105 [==============================] - 0s 35us/step - loss: 0.0442 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.7598 - val_loss: 0.0436 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7895\n",
      "Epoch 99/100\n",
      "11105/11105 [==============================] - 0s 37us/step - loss: 0.0450 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.7255 - val_loss: 0.0436 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      "11105/11105 [==============================] - 0s 38us/step - loss: 0.0444 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00 - auc_3: 0.7409 - val_loss: 0.0435 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_auc_3: 0.7756\n"
     ]
    }
   ],
   "source": [
    "iterations = 1\n",
    "\n",
    "for i in range (iterations):\n",
    "   \n",
    "    '''\n",
    "        Sample requsite tweets to balance class\n",
    "    '''\n",
    "    #df_low = df[df['Priority'] == 'Low'].sample(55)\n",
    "    #df_crit = df[df['Priority'] == 'Critical']\n",
    "    #df_c = pd.concat([df_low, df_crit])\n",
    "    #df_c = shuffle(df_c)\n",
    "    '''\n",
    "        Categorize label as 0 or 1 (1 comprises of the + class aka critical tweets)\n",
    "    '''\n",
    "    t = to_categorical(df_c)\n",
    "\n",
    "    '''\n",
    "      We tokenize the lemmatized tweets to then do word embeddings\n",
    "    '''\n",
    "    l = preProcess(df_c)\n",
    "    all_words = []\n",
    "    for tweet in l:\n",
    "        tokenize_word = word_tokenize(tweet)\n",
    "        for word in tokenize_word:\n",
    "            all_words.append(word)\n",
    "    '''\n",
    "        Getting the unique words out\n",
    "    '''\n",
    "    unique_words = set(all_words)\n",
    "\n",
    "    '''\n",
    "        Getting embedded sentences\n",
    "    '''\n",
    "    vocab_length = len(unique_words)\n",
    "    embedded_sentences = [one_hot(tweet, vocab_length) for tweet in l]\n",
    "\n",
    "    '''\n",
    "        Making the size of all embeddings equal to the longest one\n",
    "    '''\n",
    "    word_count = lambda sentence: len(word_tokenize(sentence))\n",
    "    longest_sentence = max(l, key=word_count)\n",
    "    length_long_sentence = len(word_tokenize(longest_sentence))\n",
    "    padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\n",
    "\n",
    "    '''\n",
    "        Model Paramters\n",
    "    '''\n",
    "    model = keras.Sequential()\n",
    "    model.add(Embedding(vocab_length, 16, input_length=padded_sentences.shape[1]))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    #model.add(Dense(8, activation = 'relu', input_shape = (padded_sentences.shape[1],)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "    '''\n",
    "        Compile and fit model\n",
    "    '''\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.AUC()])\n",
    "\n",
    "    history = model.fit(padded_sentences, t, batch_size=1024, epochs=100, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision =  0.7223\n",
      "Stdev Precision =  0.23877699033012187\n",
      "Mean Recall =  0.79645\n",
      "Stdev Recall =  0.3232536095871338\n",
      "Mean ROC AUC =  0.82815\n",
      "Stdev ROC AUC =  0.04027536467867174\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "precision = [0.9231, 0.5625, 0.4545, 1, 0.5455, 0.5556, 0.75, 1, 1, 0.4318]\n",
    "recall = [1, 0.3, 0.4848, 1, 1, 1, 0.2273,1, 1, 0.9524]\n",
    "AUC = [0.7821, 0.8427, 0.8946, 0.8002, 0.8383, 0.8453,0.8874, 0.8, 0.8062, 0.7847 ]\n",
    "print('Mean Precision = ', statistics.mean(precision))\n",
    "print('Stdev Precision = ', statistics.stdev(precision))\n",
    "print('Mean Recall = ', statistics.mean(recall))\n",
    "print('Stdev Recall = ', statistics.stdev(recall))\n",
    "print('Mean ROC AUC = ', statistics.mean(AUC))\n",
    "print('Stdev ROC AUC = ', statistics.stdev(AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7725922 ],\n",
       "       [0.6906314 ],\n",
       "       [0.2784298 ],\n",
       "       [0.7214121 ],\n",
       "       [0.9677178 ],\n",
       "       [0.7108212 ],\n",
       "       [0.00933427],\n",
       "       [0.05144485],\n",
       "       [0.9994686 ],\n",
       "       [0.9985454 ],\n",
       "       [0.9921698 ],\n",
       "       [0.9908632 ],\n",
       "       [0.00318617],\n",
       "       [0.9987317 ],\n",
       "       [0.9897058 ],\n",
       "       [0.24104315],\n",
       "       [0.9969379 ],\n",
       "       [0.20989063],\n",
       "       [0.95295864],\n",
       "       [0.9522515 ],\n",
       "       [0.05235607],\n",
       "       [0.74673504],\n",
       "       [0.11396532],\n",
       "       [0.99802965],\n",
       "       [0.8493108 ],\n",
       "       [0.04973009],\n",
       "       [0.95636237],\n",
       "       [0.39834592],\n",
       "       [0.992837  ],\n",
       "       [0.94512135],\n",
       "       [0.4136941 ],\n",
       "       [0.26527405],\n",
       "       [0.02235318]], dtype=float32)"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(history.validation_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-506-0a9c703d3ec2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3_Mod\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \"\"\"\n\u001b[1;32m--> 253\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_Mod\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 81\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Word Embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
