{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Vishaal\\\\Documents\\\\GitHub\\\\TREC_Distributed_Machine_Learning\\\\TREC\\\\10_Data\\\\20_Extracted Tweets'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"30_2019 B Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('Ultimate_Dataframe_TREC_2018_train_only_critical.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('Ultimate_Dataframe_TREC_2018_test_only_critical.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('Ultimate_Dataframe_TREC_2019_A-test_only_critical.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv('Ultimate_Dataframe_TREC_2019_B-test_only_critical.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2, df3, df4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Event</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Categories</th>\n",
       "      <th>Priority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87</td>\n",
       "      <td>243376377057902592</td>\n",
       "      <td>costaRicaEarthquake2012</td>\n",
       "      <td>RT @RobDavis_Wx: LANDSLIDE!... road blocked in...</td>\n",
       "      <td>['EmergingThreats']</td>\n",
       "      <td>Critical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>571</td>\n",
       "      <td>378152455005282306</td>\n",
       "      <td>floodColorado2013</td>\n",
       "      <td>RT @MarciKrivonen: Wow. RT @HemaMullur: Two tr...</td>\n",
       "      <td>['EmergingThreats']</td>\n",
       "      <td>Critical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>572</td>\n",
       "      <td>378158079579127808</td>\n",
       "      <td>floodColorado2013</td>\n",
       "      <td>RT @HemaMullur: Two trucks and one car in the ...</td>\n",
       "      <td>['EmergingThreats']</td>\n",
       "      <td>Critical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>619</td>\n",
       "      <td>378220553699463168</td>\n",
       "      <td>floodColorado2013</td>\n",
       "      <td>RT @9NEWS: #COFlood RT @TrevorHughes: Bridge c...</td>\n",
       "      <td>['EmergingThreats']</td>\n",
       "      <td>Critical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>748</td>\n",
       "      <td>396315108311199744</td>\n",
       "      <td>laAirportShooting2013</td>\n",
       "      <td>Gunshots outside #LAX. Unknown if anyone is in...</td>\n",
       "      <td>['NewSubEvent']</td>\n",
       "      <td>Critical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                  ID                    Event  \\\n",
       "0          87  243376377057902592  costaRicaEarthquake2012   \n",
       "1         571  378152455005282306        floodColorado2013   \n",
       "2         572  378158079579127808        floodColorado2013   \n",
       "3         619  378220553699463168        floodColorado2013   \n",
       "4         748  396315108311199744    laAirportShooting2013   \n",
       "\n",
       "                                               Tweet           Categories  \\\n",
       "0  RT @RobDavis_Wx: LANDSLIDE!... road blocked in...  ['EmergingThreats']   \n",
       "1  RT @MarciKrivonen: Wow. RT @HemaMullur: Two tr...  ['EmergingThreats']   \n",
       "2  RT @HemaMullur: Two trucks and one car in the ...  ['EmergingThreats']   \n",
       "3  RT @9NEWS: #COFlood RT @TrevorHughes: Bridge c...  ['EmergingThreats']   \n",
       "4  Gunshots outside #LAX. Unknown if anyone is in...      ['NewSubEvent']   \n",
       "\n",
       "   Priority  \n",
       "0  Critical  \n",
       "1  Critical  \n",
       "2  Critical  \n",
       "3  Critical  \n",
       "4  Critical  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = df['Categories']\n",
    "cat_l = {}\n",
    "for element in cat:\n",
    "    element = element[1:len(element) - 1]\n",
    "    element = element.split(',')\n",
    "    for category in element:\n",
    "        if category in cat_l.keys():\n",
    "            cat_l[category] = cat_l[category] + 1\n",
    "        else:\n",
    "            cat_l[category] = 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\" 'News'\": 216,\n",
       " \" 'Hashtags'\": 168,\n",
       " \" 'Location'\": 165,\n",
       " \" 'EmergingThreats'\": 143,\n",
       " \" 'MultimediaShare'\": 136,\n",
       " \" 'NewSubEvent'\": 124,\n",
       " \"'ThirdPartyObservation'\": 100,\n",
       " \" 'Factoid'\": 92,\n",
       " \" 'Official'\": 86,\n",
       " \"'MovePeople'\": 70,\n",
       " \" 'OriginalEvent'\": 69,\n",
       " \"'Location'\": 57,\n",
       " \"'FirstPartyObservation'\": 48,\n",
       " \"'EmergingThreats'\": 40,\n",
       " \" 'Advice'\": 39,\n",
       " \" 'Discussion'\": 20,\n",
       " \"'Weather'\": 18,\n",
       " \" 'ContextualInformation'\": 18,\n",
       " \" 'Weather'\": 17,\n",
       " \" 'ThirdPartyObservation'\": 15,\n",
       " \"'SearchAndRescue'\": 14,\n",
       " \" 'FirstPartyObservation'\": 10,\n",
       " \" 'ServiceAvailable'\": 9,\n",
       " \" 'Sentiment'\": 9,\n",
       " \"'MultimediaShare'\": 9,\n",
       " \"'Hashtags'\": 7,\n",
       " \"'NewSubEvent'\": 6,\n",
       " \"'InformationWanted'\": 6,\n",
       " \"'Factoid'\": 4,\n",
       " \"'News'\": 3,\n",
       " \"'GoodsServices'\": 3,\n",
       " \"'ServiceAvailable'\": 2,\n",
       " \" 'InformationWanted'\": 1,\n",
       " \"'Official'\": 1,\n",
       " \"'Volunteer'\": 1,\n",
       " \" 'Donations'\": 1,\n",
       " \"'Donations'\": 1}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in sorted(cat_l.items(), key=lambda item: item[1], reverse = True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Function to tokenize, remove stop words and lemmatize\n",
    "'''\n",
    "def preProcess(df):\n",
    "    df['Tweet'] = df['Tweet'].astype('str')\n",
    "    \n",
    "    token_array = []\n",
    "    for tweet in df['Tweet']:\n",
    "        token_tweet = word_tokenize(tweet)\n",
    "        token_array.append(token_tweet)\n",
    "        \n",
    "    stop_words=set(stopwords.words(\"english\"))\n",
    "    filtered_token_array=[]\n",
    "    for tweet in token_array:\n",
    "        filtered_tweet = []\n",
    "        for word in tweet:\n",
    "                if word not in stop_words:\n",
    "                    filtered_tweet.append(word)\n",
    "        filtered_token_array.append(filtered_tweet)\n",
    "        \n",
    "    lem = WordNetLemmatizer()\n",
    "    stem = PorterStemmer()\n",
    "\n",
    "    lemmatized_array=[]\n",
    "    for tweet in filtered_token_array:\n",
    "        lemmatized_tweet = []\n",
    "        for word in tweet:\n",
    "            lemmatized_tweet.append(lem.lemmatize(word,'v'))\n",
    "        lemmatized_array.append(lemmatized_tweet)\n",
    "    \n",
    "    lemmatized_array_join = []\n",
    "    for element in lemmatized_array:\n",
    "        lemmatized_array_join.append(' '.join(element))\n",
    "        \n",
    "    return (lemmatized_array_join)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2415"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Get a sense of how big the vocab size is for all the critical tweets\n",
    "'''\n",
    "l = preProcess(df)\n",
    "\n",
    "all_words = []\n",
    "\n",
    "for tweet in l:\n",
    "    tokenize_word = word_tokenize(tweet)\n",
    "    for word in tokenize_word:\n",
    "        all_words.append(word)\n",
    "\n",
    "unique_words = set(all_words)\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(':', 511),\n",
       " ('#', 431),\n",
       " ('.', 325),\n",
       " (',', 261),\n",
       " ('https', 239),\n",
       " ('@', 163),\n",
       " ('RT', 94),\n",
       " ('ymmfire', 77),\n",
       " ('!', 56),\n",
       " ('shoot', 55),\n",
       " ('ymm', 55),\n",
       " ('fire', 52),\n",
       " ('http', 50),\n",
       " ('school', 46),\n",
       " ('-', 44),\n",
       " ('...', 38),\n",
       " ('(', 36),\n",
       " ('report', 36),\n",
       " ('Paris', 36),\n",
       " ('say', 35),\n",
       " (')', 34),\n",
       " ('evacuation', 32),\n",
       " ('wildfire', 31),\n",
       " ('Colorado', 28),\n",
       " (';', 26),\n",
       " ('High', 25),\n",
       " ('2', 24),\n",
       " ('&', 24),\n",
       " ('flood', 24),\n",
       " ('Alberta', 23),\n",
       " ('Level', 23),\n",
       " ('BREAKING', 22),\n",
       " (\"'s\", 22),\n",
       " ('Ranch', 22),\n",
       " ('?', 21),\n",
       " ('STEM', 21),\n",
       " ('Highlands', 21),\n",
       " ('people', 20),\n",
       " ('School', 20),\n",
       " ('situation', 20),\n",
       " ('order', 20),\n",
       " ('sheriff', 20),\n",
       " ('Lake', 18),\n",
       " ('The', 18),\n",
       " ('amp', 18),\n",
       " ('near', 17),\n",
       " ('go', 16),\n",
       " ('evacuate', 16),\n",
       " (\"'\", 16),\n",
       " (\"''\", 15)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Get the top 50 most frequently used words in the cleaned critical tweets - aka stop words have been removed and \n",
    "    tweets have been lemmatised.\n",
    "'''\n",
    "df['Tweet'] = df['Tweet'].astype('str')\n",
    "\n",
    "token_array = []\n",
    "for tweet in df['Tweet']:\n",
    "    token_tweet = word_tokenize(tweet)\n",
    "    token_array.append(token_tweet)\n",
    "\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "filtered_token_array=[]\n",
    "for tweet in token_array:\n",
    "    filtered_tweet = []\n",
    "    for word in tweet:\n",
    "            if word not in stop_words:\n",
    "                filtered_tweet.append(word)\n",
    "    filtered_token_array.append(filtered_tweet)\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "lemmatized_array=[]\n",
    "for tweet in filtered_token_array:\n",
    "    lemmatized_tweet = []\n",
    "    for word in tweet:\n",
    "        lemmatized_tweet.append(lem.lemmatize(word,'v'))\n",
    "    lemmatized_array.append(lemmatized_tweet)\n",
    "    \n",
    "'''\n",
    "    Joining list of list into a single list\n",
    "'''\n",
    "lemmatized_array = list(itertools.chain.from_iterable(lemmatized_array))\n",
    "\n",
    "'''\n",
    "    Getting top counts\n",
    "'''\n",
    "fdist = FreqDist(lemmatized_array)\n",
    "top_fifty = fdist.most_common(50)\n",
    "top_fifty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(':', 511),\n",
       " ('#', 431),\n",
       " ('.', 325),\n",
       " (',', 261),\n",
       " ('https', 239),\n",
       " ('@', 163),\n",
       " ('RT', 94),\n",
       " ('ymmfire', 77),\n",
       " ('!', 56),\n",
       " ('shoot', 55),\n",
       " ('ymm', 55),\n",
       " ('fire', 52),\n",
       " ('http', 50),\n",
       " ('school', 46),\n",
       " ('-', 44),\n",
       " ('...', 38),\n",
       " ('(', 36),\n",
       " ('report', 36),\n",
       " ('Paris', 36),\n",
       " ('say', 35),\n",
       " (')', 34),\n",
       " ('evacuation', 32),\n",
       " ('wildfire', 31),\n",
       " ('Colorado', 28),\n",
       " (';', 26),\n",
       " ('High', 25),\n",
       " ('2', 24),\n",
       " ('&', 24),\n",
       " ('flood', 24),\n",
       " ('Alberta', 23),\n",
       " ('Level', 23),\n",
       " ('BREAKING', 22),\n",
       " (\"'s\", 22),\n",
       " ('Ranch', 22),\n",
       " ('?', 21),\n",
       " ('STEM', 21),\n",
       " ('Highlands', 21),\n",
       " ('people', 20),\n",
       " ('School', 20),\n",
       " ('situation', 20),\n",
       " ('order', 20),\n",
       " ('sheriff', 20),\n",
       " ('Lake', 18),\n",
       " ('The', 18),\n",
       " ('amp', 18),\n",
       " ('near', 17),\n",
       " ('go', 16),\n",
       " ('evacuate', 16),\n",
       " (\"'\", 16),\n",
       " (\"''\", 15)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Get the top 50 most frequently used words in the un-cleaned critical tweets - aka stop words have not been removed\n",
    "'''\n",
    "\n",
    "df['Tweet'] = df['Tweet'].astype('str')\n",
    "\n",
    "token_array = []\n",
    "for tweet in df['Tweet']:\n",
    "    token_tweet = word_tokenize(tweet)\n",
    "    token_array.append(token_tweet)\n",
    "    \n",
    "'''\n",
    "    Joining list of list into a single list\n",
    "'''\n",
    "token_array = list(itertools.chain.from_iterable(token_array))\n",
    "fdist_raw = FreqDist(token_array)\n",
    "top_fifty_raw = fdist.most_common(50)\n",
    "top_fifty_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
