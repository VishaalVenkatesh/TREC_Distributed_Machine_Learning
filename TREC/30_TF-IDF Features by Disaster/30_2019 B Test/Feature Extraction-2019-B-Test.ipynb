{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('10_Data/30_Extracted Tweets by Disaster/2019 B Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Vishaal\\\\Documents\\\\GitHub\\\\TREC_Distributed_Machine_Learning\\\\TREC'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Event</th>\n",
       "      <th>Source</th>\n",
       "      <th>User_Language</th>\n",
       "      <th>Event_Decrption</th>\n",
       "      <th>Categories</th>\n",
       "      <th>Priority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1134239576111771648</td>\n",
       "      <td>Tolkoâ€™s Athabasca division in Slave Lake, Al...</td>\n",
       "      <td>albertaWildfires2019</td>\n",
       "      <td>&lt;a href=\"https://dlvrit.com/\" rel=\"nofollow\"&gt;d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The 2019 Alberta wildfires were described by N...</td>\n",
       "      <td>['Location', 'News']</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1131161667398774784</td>\n",
       "      <td>'Heartbroken' Alberta woman describes wildfire...</td>\n",
       "      <td>albertaWildfires2019</td>\n",
       "      <td>&lt;a href=\"https://www.awayshare.com/\" rel=\"nofo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The 2019 Alberta wildfires were described by N...</td>\n",
       "      <td>['Irrelevant']</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1134148540719017984</td>\n",
       "      <td>There is so much smoke from the high level fir...</td>\n",
       "      <td>albertaWildfires2019</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The 2019 Alberta wildfires were described by N...</td>\n",
       "      <td>['Irrelevant']</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1133705772246786049</td>\n",
       "      <td>Waking to smoky sunrise in Calgary today - eer...</td>\n",
       "      <td>albertaWildfires2019</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The 2019 Alberta wildfires were described by N...</td>\n",
       "      <td>['Irrelevant']</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1134018172309295104</td>\n",
       "      <td>BREAKING: Hundreds of evacuees in Calling Lake...</td>\n",
       "      <td>albertaWildfires2019</td>\n",
       "      <td>&lt;a href=\"https://about.twitter.com/products/tw...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The 2019 Alberta wildfires were described by N...</td>\n",
       "      <td>['Location', 'Factoid', 'Hashtags', 'News']</td>\n",
       "      <td>Critical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1                   ID  \\\n",
       "0           0             0  1134239576111771648   \n",
       "1           1             1  1131161667398774784   \n",
       "2           2             2  1134148540719017984   \n",
       "3           3             3  1133705772246786049   \n",
       "4           4             4  1134018172309295104   \n",
       "\n",
       "                                               Tweet                 Event  \\\n",
       "0  Tolkoâ€™s Athabasca division in Slave Lake, Al...  albertaWildfires2019   \n",
       "1  'Heartbroken' Alberta woman describes wildfire...  albertaWildfires2019   \n",
       "2  There is so much smoke from the high level fir...  albertaWildfires2019   \n",
       "3  Waking to smoky sunrise in Calgary today - eer...  albertaWildfires2019   \n",
       "4  BREAKING: Hundreds of evacuees in Calling Lake...  albertaWildfires2019   \n",
       "\n",
       "                                              Source  User_Language  \\\n",
       "0  <a href=\"https://dlvrit.com/\" rel=\"nofollow\">d...            NaN   \n",
       "1  <a href=\"https://www.awayshare.com/\" rel=\"nofo...            NaN   \n",
       "2  <a href=\"http://twitter.com/download/iphone\" r...            NaN   \n",
       "3  <a href=\"http://twitter.com/download/android\" ...            NaN   \n",
       "4  <a href=\"https://about.twitter.com/products/tw...            NaN   \n",
       "\n",
       "                                     Event_Decrption  \\\n",
       "0  The 2019 Alberta wildfires were described by N...   \n",
       "1  The 2019 Alberta wildfires were described by N...   \n",
       "2  The 2019 Alberta wildfires were described by N...   \n",
       "3  The 2019 Alberta wildfires were described by N...   \n",
       "4  The 2019 Alberta wildfires were described by N...   \n",
       "\n",
       "                                    Categories  Priority  \n",
       "0                         ['Location', 'News']      High  \n",
       "1                               ['Irrelevant']       Low  \n",
       "2                               ['Irrelevant']       Low  \n",
       "3                               ['Irrelevant']       Low  \n",
       "4  ['Location', 'Factoid', 'Hashtags', 'News']  Critical  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('fire_TREC_2019_B_test.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Converting tweet column to str\n",
    "'''\n",
    "df['Tweet'] = df['Tweet'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Generalise process to all files. Maybe later\n",
    "'''\n",
    "event_type = ['Floods', 'Earthquake', 'Bushfire', 'Bombings', 'Tornado', 'Attack', 'SchoolShooting', 'typhoon' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'Heartbroken\",\n",
       " \"'\",\n",
       " 'Alberta',\n",
       " 'woman',\n",
       " 'describes',\n",
       " 'wildfire',\n",
       " 'evacuation',\n",
       " \"'Heartbroken\",\n",
       " \"'\",\n",
       " 'Alberta',\n",
       " 'woman',\n",
       " 'describes',\n",
       " 'wildfire',\n",
       " 'evacuation',\n",
       " '&',\n",
       " 'lt',\n",
       " ';',\n",
       " 'p',\n",
       " '&',\n",
       " 'gt',\n",
       " ';',\n",
       " '&',\n",
       " 'lt',\n",
       " ';',\n",
       " 'img',\n",
       " 'src=',\n",
       " \"''\",\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/6JN9HiOCaK',\n",
       " \"''\",\n",
       " '/',\n",
       " '&',\n",
       " 'gt',\n",
       " ';',\n",
       " '&',\n",
       " 'lt',\n",
       " ';',\n",
       " '/p',\n",
       " '&',\n",
       " 'gt',\n",
       " ';',\n",
       " 'The',\n",
       " 'town',\n",
       " 'of',\n",
       " 'High',\n",
       " 'Level',\n",
       " ',',\n",
       " 'Alta.',\n",
       " ',',\n",
       " 'and',\n",
       " 'a',\n",
       " 'nearby',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/EEdMYc3EYi',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/PPApbrj1v1']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Tokenize each tweet into words. Note we haven't yet removed stop words\n",
    "'''\n",
    "token_array = []\n",
    "for tweet in df['Tweet']:\n",
    "    token_tweet = word_tokenize(tweet)\n",
    "    token_array.append(token_tweet)\n",
    "                       \n",
    "token_array[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'Heartbroken\",\n",
       " \"'\",\n",
       " 'Alberta',\n",
       " 'woman',\n",
       " 'describes',\n",
       " 'wildfire',\n",
       " 'evacuation',\n",
       " \"'Heartbroken\",\n",
       " \"'\",\n",
       " 'Alberta',\n",
       " 'woman',\n",
       " 'describes',\n",
       " 'wildfire',\n",
       " 'evacuation',\n",
       " '&',\n",
       " 'lt',\n",
       " ';',\n",
       " 'p',\n",
       " '&',\n",
       " 'gt',\n",
       " ';',\n",
       " '&',\n",
       " 'lt',\n",
       " ';',\n",
       " 'img',\n",
       " 'src=',\n",
       " \"''\",\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/6JN9HiOCaK',\n",
       " \"''\",\n",
       " '/',\n",
       " '&',\n",
       " 'gt',\n",
       " ';',\n",
       " '&',\n",
       " 'lt',\n",
       " ';',\n",
       " '/p',\n",
       " '&',\n",
       " 'gt',\n",
       " ';',\n",
       " 'The',\n",
       " 'town',\n",
       " 'High',\n",
       " 'Level',\n",
       " ',',\n",
       " 'Alta.',\n",
       " ',',\n",
       " 'nearby',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/EEdMYc3EYi',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/PPApbrj1v1']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Will remove stop words from tweet. We still have to look into removing punctuation marks.\n",
    "'''\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "filtered_token_array=[]\n",
    "for tweet in token_array:\n",
    "    filtered_tweet = []\n",
    "    for word in tweet:\n",
    "        if word not in stop_words:\n",
    "            filtered_tweet.append(word)\n",
    "    filtered_token_array.append(filtered_tweet)\n",
    "    \n",
    "filtered_token_array[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'heartbroken\",\n",
       " \"'\",\n",
       " 'alberta',\n",
       " 'woman',\n",
       " 'describ',\n",
       " 'wildfir',\n",
       " 'evacu',\n",
       " \"'heartbroken\",\n",
       " \"'\",\n",
       " 'alberta',\n",
       " 'woman',\n",
       " 'describ',\n",
       " 'wildfir',\n",
       " 'evacu',\n",
       " '&',\n",
       " 'lt',\n",
       " ';',\n",
       " 'p',\n",
       " '&',\n",
       " 'gt',\n",
       " ';',\n",
       " '&',\n",
       " 'lt',\n",
       " ';',\n",
       " 'img',\n",
       " 'src=',\n",
       " \"''\",\n",
       " 'http',\n",
       " ':',\n",
       " '//t.co/6jn9hiocak',\n",
       " \"''\",\n",
       " '/',\n",
       " '&',\n",
       " 'gt',\n",
       " ';',\n",
       " '&',\n",
       " 'lt',\n",
       " ';',\n",
       " '/p',\n",
       " '&',\n",
       " 'gt',\n",
       " ';',\n",
       " 'the',\n",
       " 'town',\n",
       " 'high',\n",
       " 'level',\n",
       " ',',\n",
       " 'alta.',\n",
       " ',',\n",
       " 'nearbi',\n",
       " 'http',\n",
       " ':',\n",
       " '//t.co/eedmyc3eyi',\n",
       " 'http',\n",
       " ':',\n",
       " '//t.co/ppapbrj1v1']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    We will now do stemming. This is the process of removing different forms of the same word and will\n",
    "    resort to the root word. For example, connection, connected, connecting word reduce to a common \n",
    "    word \"connect\".\n",
    "'''\n",
    "ps = PorterStemmer()\n",
    "stemmed_array=[]\n",
    "for tweet in filtered_token_array:\n",
    "    stemmed_tweet = []\n",
    "    for word in tweet:\n",
    "        stemmed_tweet.append(ps.stem(word))\n",
    "    stemmed_array.append(stemmed_tweet)\n",
    "    \n",
    "stemmed_array[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    We now do lemmatization. This is like stemming but more effective apparently as it does a dictionary lookup. For \n",
    "    instance a relation between the words good and better may be made in lemmatisation but not in stemming.\n",
    "    \n",
    "    Lemmatization is much better from a cursory look. Words like earthquake are being cut down to earthquak \n",
    "    when using stemming. \n",
    "'''\n",
    "lem = WordNetLemmatizer()\n",
    "stem = PorterStemmer()\n",
    "\n",
    "lemmatized_array=[]\n",
    "for tweet in filtered_token_array:\n",
    "    lemmatized_tweet = []\n",
    "    for word in tweet:\n",
    "        lemmatized_tweet.append(lem.lemmatize(word,'v'))\n",
    "    lemmatized_array.append(lemmatized_tweet)\n",
    "\n",
    "lemmatized_array_join = []\n",
    "for element in lemmatized_array:\n",
    "    lemmatized_array_join.append(''.join(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    DTM to get bag of words\n",
    "'''\n",
    "\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1))\n",
    "text_counts= cv.fit_transform(lemmatized_array_join)\n",
    "text_counts_dense = text_counts.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    DTM to get TF-IDF features\n",
    "'''\n",
    "\n",
    "tf=TfidfVectorizer()\n",
    "text_tf= tf.fit_transform(lemmatized_array_join)\n",
    "text_tf_dense = text_tf.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Converting TF-IDF to list of lists. Then we play around with the datatypes to get features as a dense\n",
    "    list of numpy arrays l_features. We also get the actual word names that are used as features.\n",
    "'''\n",
    "words = tf.get_feature_names()\n",
    "M = text_tf.tolil()\n",
    "l_features = []\n",
    "for i in range(M.shape[0]):\n",
    "    l_features.append(np.array(M[i].todense())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Converting to dataframe\n",
    "'''\n",
    "df_features = pd.DataFrame(l_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Combining both the dataframes - we now have the TF-IDF features and all the other stuff we had before\n",
    "'''\n",
    "df_combined = pd.concat([df,df_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    List of columns with original columns and word features\n",
    "'''\n",
    "cols = list(df.columns) + words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Rename columns in original dataframe with new column names\n",
    "'''\n",
    "df_combined = df_combined.rename(columns={x:y for x,y in zip(df_combined.columns,cols)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Creating a function to the above to all the files...\n",
    "'''\n",
    "def preProcess(file):\n",
    "    df = pd.read_csv(file+'_TREC_2019_B_test.csv')\n",
    "    df['Tweet'] = df['Tweet'].astype('str')\n",
    "    \n",
    "    token_array = []\n",
    "    for tweet in df['Tweet']:\n",
    "        token_tweet = word_tokenize(tweet)\n",
    "        token_array.append(token_tweet)\n",
    "        \n",
    "    stop_words=set(stopwords.words(\"english\"))\n",
    "    filtered_token_array=[]\n",
    "    for tweet in token_array:\n",
    "        filtered_tweet = []\n",
    "        for word in tweet:\n",
    "                if word not in stop_words:\n",
    "                    filtered_tweet.append(word)\n",
    "        filtered_token_array.append(filtered_tweet)\n",
    "        \n",
    "    lem = WordNetLemmatizer()\n",
    "    stem = PorterStemmer()\n",
    "\n",
    "    lemmatized_array=[]\n",
    "    for tweet in filtered_token_array:\n",
    "        lemmatized_tweet = []\n",
    "        for word in tweet:\n",
    "            lemmatized_tweet.append(lem.lemmatize(word,'v'))\n",
    "        lemmatized_array.append(lemmatized_tweet)\n",
    "    \n",
    "    lemmatized_array_join = []\n",
    "    for element in lemmatized_array:\n",
    "        lemmatized_array_join.append(''.join(element))\n",
    "        \n",
    "    return (lemmatized_array_join)\n",
    "    \n",
    "    \n",
    "def tfidf(file, corpus):\n",
    "    df = pd.read_csv(file+'_TREC_2019_B_test.csv')\n",
    "    tf=TfidfVectorizer()\n",
    "    text_tf= tf.fit_transform(corpus)\n",
    "    text_tf_dense = text_tf.todense()\n",
    "    \n",
    "    words = tf.get_feature_names()\n",
    "    M = text_tf.tolil()\n",
    "    l_features = []\n",
    "    for i in range(M.shape[0]):\n",
    "        l_features.append(np.array(M[i].todense())[0])\n",
    "        \n",
    "    df_features = pd.DataFrame(l_features)\n",
    "    \n",
    "    df_combined = pd.concat([df,df_features], axis=1)\n",
    "    \n",
    "    cols = list(df.columns) + words\n",
    "    \n",
    "    df_combined_1 = df_combined.rename(columns={x:y for x,y in zip(df_combined.columns,cols)})\n",
    "    \n",
    "    return (df_combined_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = ['Flood', 'Earthquake', 'fire', 'Shooting', 'cyclone' ]\n",
    "\n",
    "for disaster in event_type:\n",
    "    feature = preProcess(disaster)\n",
    "    df = tfidf(disaster, feature)\n",
    "    df.to_csv(disaster+'_tfidf_features_2019_B_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
